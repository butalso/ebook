# MySQL篇

## 数据库基础知识
### 使用数据库系统的好处
1. 数据库系统中数据的共享性高，大大减少数据冗余，节约存储空间，避免数据之间的不相容性与不一致性，使得数据库系统弹性大，易于扩充。
2. 数据库系统的数据独立性高，把数据的定义从程序中分离出去，而存取数据的方法又由数据库管理系统负责提供，从而简化了应用程序的编制，大大减少了应用程序的维护和修改。
3. 数据库管理系统提供了数据的安全性保护、数据的完整性检查、并发控制、数据库恢复等数据控制功能，保证了数据的完整性和安全性，并在多用户同时使用数据库时进行并发控制，在发生故障后对数据库进行恢复。

### SQL语句分类
DQL、DDL、DML、DCL

### 超键、候选键、主键、外键
* 超键(super key):在关系中能唯一标识元组的属性集称为关系模式的超键
* 候选键(candidate key):不含有多余属性的超键称为候选键
* 主键(primary key):用户选作元组标识的一个候选键程序主键
* 外键(foreign key)如果关系模式R1中的某属性集不是R1的主键，而是另一个关系R2的主键则该属性集是关系模式R1的外键。

### SQL约束有哪几种
primary key、unqiue key、foreign key、default、not null、 check

### 数据库三大范式
1. 第一范式(确保每列保持原子性): 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。
2. 第二范式(确保表中的每列都和主键相关): 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中
3. 第三范式(确保每列都和主键列直接相关,而不是间接相关): 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。

### MySQL的binlog形式
* statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
* row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
* mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。

### MySQL的几种日志文件
* 错误日志：记录出错信息，也记录一些警告信息或者正确的信息。
* 查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。
* 慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。
* 二进制日志：记录对数据库执行更改的所有操作。
* 中继日志：中继日志也是二进制日志，在主从复制中，从服务器将来自主服务器的二进制日志保存为中继日志，在主从复制中起到缓冲的作用。
* 事务日志：重做日志redo和回滚日志undo

### 二进制日志和重做日志的区别
* 重做日志是在InnoDB存储引擎层产生，而二进制日志是在MySQL数据库的上层产生的，并且二进制日志不仅仅针对于InnoDB存储引擎， MySQL数据库中的任何存储引擎对于数据库的更改都会产生二进制日志
* 两种日志记录的内容形式不同。MySQL数据库上层的二进制日志是一种逻辑日志， 其记录的是对应的SQL语句。 而InnoDB存储引擎层面的重做日志是物理格式日志， 其记录的是对于每个页的修改
* 两种日志记录写入磁盘的时间点不同，二进制日志只在事务提交完成后进行一次写入。而InnoDB存储引擎的重做日志在事务进行中不断地被写入， 这表现为日志并不是随事务提交的顺序进行写入的
* 重做日志可以幂等的执行，二进制日志不可以

### drop、delete、truncate的区别
* 从类型上看， drop、truncate属于DDL，delete属于DML，因此，drop、truncate不可回滚（DDL是隐形提交的），delete可回滚
* 从删除内容上看，drop会删掉该表的所有数据，包括表结构、表数据、表索引、权限；truncate只会删除表中全部数据；delete只会删除表中部分或全部数据
* 从执行速度上看，drop > truncate > delete

### 触发器
触发器的作用是在执行INSERT、DELETE和UPDATE命令之前或之后自动调用SQL命令或存储过程。最多可以为一个表建立6个触发器， 即分别为INSERT、UPDATE、 DELETE的BEFORE和AFTER各定义一个。 BEFORE和AFTER代表触发器发生的时间， 表示是在每行操作的之前发生还是之后发生。 当前MySQL数据库只支持FOR EACH ROW的触发方式，即按每行记录进行触发， 

### in VS exists
#### 分析
* IN()只执行一次，它查出B表中的所有id字段并缓存起来。之后，检查A表的id是否与B表中的id相等，如果相等则将A表的记录加入结果集中，直到遍历完A表的所有记录。
* exists()会执行A.length次，它并不缓存exists()结果集，因为exists()结果集的内容并不重要，重要的是其内查询语句的结果集空或者非空，空则返回false，非空则返回true。
#### 结论
* 当A表数据与B表数据一样大时，in与exists效率差不多，可任选一个使用。
* IN适合于外表大而内表小的情况；
* EXISTS适合于外表小而内表大的情况。
* `not in`和`not exists`：如果查询语句使用了`not in`，那么内外表都进行全表扫描，没有用到索引；而`not extsts`的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。

## 存储引擎
### MySQL存储引擎InnoDB和MyISAM的比较
1. `InnoDB`支持事务，MyISAM不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
2. InnoDB最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是`MySQL`将默认存储引擎从 MyISAM变成InnoDB的重要原因之一；
3. InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；  
4. InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此`InnoDB`必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而`MyISAM`是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 
5. InnoDB 不保存表的具体行数，执行`select count(*) from table`时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；    

### 如何判断选择InnoDB还是MySIAM
1. 是否要支持事务，如果要请选择 InnoDB，如果不需要可以考虑 MyISAM；
2. 如果表中绝大多数都只是读查询，可以考虑 MyISAM，如果既有读写也挺频繁，请使用InnoDB。
3. 系统奔溃后，MyISAM恢复起来更困难，能否接受，不能接受就选 InnoDB； 
4. MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的。如果你不知道用什么存储引擎，那就用InnoDB，至少不会差。

### InnoDB引擎的特性
#### 插入缓冲
对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个`Insert Buffer`对象中，好似欺骗。数据库这个非聚集的索引已经插到叶子节点，而实际并没有，只是存放在另一个位置。然后再以一定的频率和情况进行`Insert Buffer`和辅助索引页子节点的merge（合并）操作， 这时通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚集索引插入的性能。

###### 插入缓冲需要同时满足以下两个条件：
* 索引是辅助索引（secondary index）
* 索引不是唯一（unique）的，如果是唯一的，需要校验唯一性，如果去查找就会发生随机读，失去插入缓冲的意义

##### 插入缓冲问题：
* 大量使用插入缓冲，如果MySQL宕机，恢复可能需要较长的时间
* 写密集情况下，占用过多内存

#### 两次写
* 两次写应用在当页发生部分写失效时，使用页的副本先还原该页，然后通过重做日志进行重做。
* `double write`由两部分组成，一部分是内存中的`double write buffer`，大小为2MB，另一部分是物理磁盘上共享表空间中连续的128个页，即2个区（extent），大小同样为2MB。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过`memcpy`函数将脏页先复制到内存中的`double write buffer`，之后通过`double write buffer`再分两次，每次1MB顺序地写入共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘， 避免缓冲写带来的问题。在这个过程中，因为`double write`页是连续的，因此这个过程是顺序写的，开销并不是很大。在完成`double write`页的写入后，再将`double write buffer`中的页写入各个表空间文件中，此时的写入则是离散的

#### 自适应哈希索引
* 哈希（hash） 是一种非常快的查找方法， 在一般情况下这种查找的时间复杂度为O(1)， 即一般仅需要一次查找就能定位数据。 而B+树的查找次数， 取决于B+树的高度， 在生产环境中， B+树的高度一般为3～4层， 故需要3～4次的查询。
* AHI是通过缓冲池的B+树页构造而来， 因此建立的速度很快， 而且不需要对整张表构建哈希索引。InnoDB存储引擎会自动根据访问频率和模式来自动地为某些热点页建立哈希索引。
* AHI有一个要求， 即对这个页的连续访问模式必须是一样的哈希索引只能用来搜索等值的查询， 而对于其他查找类型， 如范围查找， 是不能使用哈希索引的，

#### 异步IO
#### 刷新邻接页
当刷新一个脏页时， InnoDB存储引擎会检测该页所在区（extent） 的所有页， 如果是脏页， 那么一起进行刷新。 通过AIO可以将多个IO写入操作合并为一个IO操作

## 表
### 什么是视图
视图（View）是一个命名的虚表，它由一个SQL查询来定义， 可以当做表使用。 与持久表（permanent table）不同的是， 视图中的数据没有实际的物理存储。对于一些应用程序， 程序本身不需要关心基表（base table）的结构， 只需要按照视图定义来取数据或更新数据，因此， 视图同时在一定程度上起到一个安全层的作用。

### 视图的使用场景有哪些？
视图根本用途：简化sql查询，提高开发效率。如果说还有另外一个用途那就是兼容老的表结构。下面是视图的常见使用场景：
* 重用SQL语句；
* 简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节；
* 使用表的组成部分而不是整个表；
* 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限；
* 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据

### 使用视图的优缺点
#### 优点
* 查询简单化。视图能简化用户的操作
* 数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护
* 逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性
#### 缺点
* 性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。
* 修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的这些视图有如下特征：
  1. 有UNIQUE等集合操作符的视图。
  2. 有GROUP BY子句的视图。
  3. 有诸如AVG\SUM\MAX等聚合函数的视图。 
  4. 使用DISTINCT关键字的视图。
  5. 连接表的视图（其中有些例外）

### MySQL如何实现物化视图
通过触发器， 在MySQL数据库中实现了类似物化视图的功能。但是MySQL数据库本身并不支持物化视图

### 什么是存储过程？有哪些优缺点？
存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。
#### 优点
* 存储过程是预编译过的，执行效率高。
* 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。
* 安全性高，执行存储过程需要有一定权限的用户。
* 存储过程可以重复使用，减少数据库开发人员的工作量。

#### 缺点
* 调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。
* 移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。
* 重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。
* 如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。


## 索引
### 索引的基本原理
索引的目的在于提高查询效率，可以类比字典，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。

### 索引算法有哪些
#### 分类
按数据存储方式，可分为聚簇索引、非聚簇索引
* 聚簇索引：表数据按照索引的顺序来存储的，一张表上只能创建一个聚集索引
* 非聚簇索引：表数据存储顺序与索引顺序无关，通过指针跳转到数据，数据存储在堆文件上
* 覆盖索引：聚簇索引&非聚簇索引中间态，指向聚簇索引

#### MySQL为什么选择B+树， 而不是二叉树、红黑树、B树
##### B类树 VS 红黑树
MySQL中的数据一般是放在磁盘中的，读取数据的时候需要找到数据在磁盘中的位置，才能够读取。一般来说，数据存取由存取磁盘的时间和CPU计算时间这两部分构成，而CPU的速度非常快，所以数据存取主要取决于访问磁盘的次数，IO次数越少，则速度越快。B/B+树是为了磁盘或其它存储设备而设计的一种平衡多路查找树(相对于二叉，B树每个内节点有多个分支)，与红黑树相比，在相同的的节点的情况下，一颗B/B+树的高度远远小于红黑树的高度。因此，关键字总数相同的情况下，B树可以比二叉树、红黑树存取速度更快。
##### B树 VS B+树
1. B+树索引的空间利用率更高，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；
2. B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 
// 3. B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。即B树只适合随机检索，而B+树同时支持随机检索和顺序检索；

#### B+树索引
B+树是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量

#### 哈希索引
和B树索引相比，Hash索引查询更快，但是其也有一些问题
1. 和Hash索引相比，B+树更适合作为外存索引（Extensible Hash Tables和 Linear Hash Tables可以作为外存索引）
2. 不支持范围查询；在组合键作为索引的情况下，无法使用部分键值做查询；不能通过索引进行键值排序；
3. 由于Hash值有可能冲突，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较。

#### 位图索引
位图索引是针对那些低基数，并且值不经常改变的列的。
位图索引并不适用于OLTP业务，OLTP一般都是有大量的并发事务来修改同样的数据。bitmap主要就是设计来为数据仓库服务的，即应用于低基数超级大数据量查询服务，而且只用在where clause里包含and ,or,not,或equalityqueries（比如在and和or条件的查询，在把bit转换成rowid以前，就能很快的得到相应的boolean操作）。

### InnoDB索引设计原则
1. 尽量选择区分度高的列作为索引
2. 选择唯一性索引： 唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。例如，学生表中学号是具有唯一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存在同名现象，从而降低查询速度。
3. 为经常需要排序、分组和联合操作的字段建立索引: 经常需要ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。
4. 为常作为查询条件的字段建立索引: 如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度。
5. 限制索引的数目: 索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。
6. 尽量使用数据量少的索引: 如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR（100）类型的字段进行全文检索需要的时间肯定要比对CHAR（10）类型的字段需要的时间要多。
7. 尽量使用前缀来索引: 如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。
8. 删除不再使用或者很少使用的索引: 表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。

### 使用索引查询一定能提高性能吗
使用索引不一定会提高性能，考虑一下两种点：
1. 索引需要空间来存储，也需要定期维护，每当有记录在表中增减或索引列被修改时，索引本身也会被修改，这就降低了数据的增删改速度。
2. 当进行一个范围查找时，如果返回结果集占整个数据集较大一部分时，一般是20%左右，如果采用索引，将会进行大量随机IO，这时候，优化器一般也会采用全表扫描。

### 非聚簇索引一定会回表查询吗？
不一定，如果从辅助索引中就可以得到查询的记录，那就不需要查询聚集索引中的记录，即覆盖索引。使用覆盖索引的一个好处是辅助索引不包含整行记录的所有信息，故其大小要远小于聚集索引，因此可以减少大量的IO操作。

### 联合索引是什么？为什么需要注意联合索引中的顺序？
MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。

## 锁
### MySQL有几种锁类型
#### 锁粒度分类
行级锁、页级锁、表级锁
#### 锁类别分类
共享锁、排它锁

### InnoDB锁类型
#### 共享锁&互斥锁
#### 意向锁
1. InnoDB 支持多粒度锁，特定场景下，行级锁可以与表级锁共存。
2. 意向锁之间互不排斥，但除了`IS`与S 兼容外，意向锁与共享锁、排他锁互斥。
3. IX（意向共享锁），IS（意向排它锁）是表级锁，不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突。
4. 意向锁在保证并发性的前提下，实现了行锁和表锁共存且满足事务隔离性的要求。

#### 自增长&锁
自增锁是一种特殊的表级锁，主要用于事务中插入自增字段，也就是我们最常用的自增主键id。
##### 自增长分类
* insert-like：指所有的插入语句，如insert，replace，insert—select,replace—select,load data等
* simple insert：指能在插入之前就确定插入行数的语句。这些语句包含insert、replace等，需要注意的是：simple inserts不包含insert on duplicater key update这类SQL语句
* bulk insert：指在插入之前不能确定得到插入行数的语句，如insert—select，replace–select，load data
* mixed-mode inserts：指插入中有一部分的值是自增长的，有一部分是确定的。如insert on duplicater key update这类SQL语句
##### 自增长锁分类innodb_autoinc_lock_mode
* 0：通过表锁的AUTO-INC Locking方式，因为有了新的自增长实现方式，0这个选项不应该是新版用户的首选项
* 1：默认值。对于simple inserts，该值会用互斥量去对内存中的计数器进行累加的操作，对于bulk inserts，还是使用传统表锁的AUTO-INC Locking方式。在这种配置下，如果不考虑回滚操作，对于自增长列的增长还是连续的，并且在这种方式下，statement-based方式的replication还是能很好地工作。需要注意的是，如果已经使用AUTO-INC Locking方式去产生自增长的值，而这时需要进行simple inserts的操作时，还是需要等待AUTO-INC Locking的释放
* 2：所有的insert-like自增长的产生都是通过互斥量，而不是通过AUTO-INC Locking的方式，显然这是性能最高的方式。然而会带来一定的问题。因为并发插入的存在，在每次插入时，自增长的值可能不是连续的。最重要的是，基于Statment-base replication会出现问题。因此，使用这个模式，任何时候都应该使用row-base replication，这样才能保证最大的并发性能及replication主从数据的一致。

#### 外键&锁
* 对于外键值的插入或更新，首先需要查询父表中的记录，即SELECT父表。但是对于父表的SELECT操作，不是使用一致性非锁定读的方式，因为这样会发生数据不一致的问题，因此这时使用的是`SELECT…LOCK IN SHARE MODE`方式，即主动对父表加一个S锁。如果这时父表上已经这样加X锁，子表上的操作会被阻塞
* 在InnoDB存储引擎中，对于一个外键列，如果没有显式地对这个列加索引，InnoDB存储引擎自动对其加一个索引，因为这样可以避免表锁——这比Oracle数据库做得好，Oracle数据库不会自动添加索引，用户必须自己手动添加，这也导致了Oracle数据库中可能产生死锁。 

#### 行级锁
InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。InnoDB有三种行锁的算法：
1. `Record Lock`（行锁）：单个行记录上的锁，锁定一个记录上的索引，而不是记录本身。如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引。
2. `Gap Lock`：间隙锁，锁定一个范围，但不包括记录本身，锁定索引之间的间隙，但是不包含索引本身。**间隙锁之间是不会互斥的，是Innodb在\color{red}{可重复读}提交下为了解决幻读问题时引入的锁机制**
3. `Next-Key Lock`（临键锁）：`Record Lock + Gap Lock`，锁定一个范围，并且锁定记录本身。当查询的索引含有唯一属性的时候，`Next-Key Lock`会进行优化，将其降级为`Record Lock`，即仅锁住索引本身，不是范围。MVCC不能解决幻读的问题，`Next-Key Locks`就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用`MVCC + Next-Key Locks`可以解决幻读问题。

#### 锁问题
* 脏读：在不同的事务下，当前事务可以读到另外事务未提交的数据，简单来说就是可以读到脏数据
* 不可重复读：在一个事务内多次读取同一数据集合。在这个事务还没有结束时，另外一个事务也访问该同一数据集合，并做了一些DML操作。因此，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的情况，这种情况称为不可重复读。
* 幻读：当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还存在没有修改的数据行，就好象发生了幻觉一样。
* 丢失更新：在当前数据库的任何隔离级别下，都不会导致数据库理论意义上的丢失更新问题。这是因为，即使是`READ UNCOMMITTED`的事务隔离级别，对于行的DML操作，需要对行或其他粗粒度级别的对象加锁。不过对于一些需要先把某个数值查询出来进行校验，然后再进行更新的操作，在用户并发操作的情况下，可能会出现丢失更新。这时候，在校验数据这一步应该给数据加上排它锁，即Select...For Update

### 死锁
#### 什么是死锁
是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。

#### 产生死锁的四个必要条件：
* 互斥条件：一个资源每次只能被一个进程使用。
* 占有且等待：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
* 不可强行占有：进程已获得的资源，在末使用完之前，不能强行剥夺。
* 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。

这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。

#### 死锁预防
死锁的预防是保证系统不进入死锁状态的一种策略。它的基本思想是要求进程申请资源时遵循某种协议，从而打破产生死锁的四个必要条件中的一个或几个，保证系统不会进入死锁状态。
##### 打破互斥条件
即允许进程同时访问某些资源。但是，有的资源是不允许被同时访问的，像打印机等等，这是由资源本身的属性所决定的。所以，这种办法并无实用价值。
##### 打破不可抢占条件
即允许进程强行从占有者那里夺取某些资源。就是说，当一个进程已占有了某些资源，它又申请新的资源，但不能立即被满足时，它必须释放所占有的全部资源，以后再重新申请。它所释放的资源可以分配给其它进程。这就相当于该进程占有的资源被隐蔽地强占了。这种预防死锁的方法实现起来困难，会降低系统性能。    
##### 打破占有且申请条件
可以实行资源预先分配策略。即进程在运行前一次性地向系统申请它所需要的全部资源。如果某个进程所需的全部资源得不到满足，则不分配任何资源，此进程暂不运行。只有当系统能够满足当前进程的全部资源需求时，才一次性地将所申请的资源全部分配给该进程。由于运行的进程已占有了它所需的全部资源，所以不会发生占有资源又申请资源的现象，因此不会发生死锁。但是，这种策略也有如下缺点：
1. 在许多情况下，一个进程在执行之前不可能知道它所需要的全部资源。这是由于进程在执行时是动态的，不可预测的；
2. 资源利用率低。无论所分资源何时用到，一个进程只有在占有所需的全部资源后才能执行。即使有些资源最后才被该进程用到一次，但该进程在生存期间却一直占有它们，造成长期占着不用的状况。这显然是一种极大的资源浪费；
3. 降低了进程的并发性。因为资源有限，又加上存在浪费，能分配到所需全部资源的进程个数就必然少了。    
##### 打破循环等待条件
实行资源有序分配策略。采用这种策略，即把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路。所有进程对资源的请求必须严格按资源序号递增的顺序提出。进程占用了小号资源，才能申请大号资源，就不会产生环路，从而预防了死锁。这种策略与前面的策略相比，资源的利用率和系统吞吐量都有很大提高，但是也存在以下缺点：
1. 限制了进程对资源的请求，同时给系统中所有资源合理编号也是件困难事，并增加了系统开销；
2. 为了遵循按编号申请的次序，暂不使用的资源也需要提前申请，从而增加了进程对资源的占用时间。

#### 死锁避免（银行家算法）
死锁预防是排除死锁的静态策略，它使产生死锁的四个必要条件不能同时具备，从而对进程申请资源的活动加以限制，以保证死锁不会发生。死锁避免则是排除死锁的动态策略。
#### 死锁检测
#### 死锁解除

### MVCC
#### 什么是MVCC
多版本并发控制（`Multi-Version Concurrency Control, MVCC`）是MySQL的InnoDB存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。可以认为MVCC是行级锁一个变种，但是他很多情况下避免了加锁操作，开销更低。虽然不同数据库的实现机制有所不同，但大都实现了非阻塞的读操作（读不用加锁，结合next-key lock能避免出现不可重复读和幻读），写操作也只锁定必要的行（写必须加锁，否则不同事务并发写会导致数据不一致）。

#### MVCC实现机制
InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现。这两个列一个保存了行的创建时间，一个保存行的过期时间（删除时间）。当然存储的并不是真实的时间而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动新增。事务开始时刻的系统版本号会作为事务的版本号，用来查询到每行记录的版本号进行比较。

##### 版本号
* 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。
* 事务版本号：事务开始时的系统版本号。

##### 隐藏的列
MVCC在每行记录后面都保存着两个隐藏的列，用来存储两个版本号：
* 创建版本号：创建一行数据时，将当前系统版本号作为创建版本号赋值。
* 删除版本号：删除一行数据时，将当前系统版本号作为删除版本号赋值。

#### REPEATABLE READ（可重复读）隔离级别下MVCC如何工作：
当开始新一个事务时，该事务的版本号肯定会大于当前所有数据行快照的创建版本号，理解这一点很关键。
##### SELECT
InnoDB会根据以下条件检查每一行记录：
1. InnoDB只查找版本早于当前事务版本的数据行，这样可以确保事务读取的行要么是在开始事务之前已经存在要么是事务自身插入或者修改过的，在事务开始之后才插入的行，事务不会看到。
2. 行的删除版本号要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行在事务开始之前未被删除，在事务开始之前就已经过期的数据行，该事务也不会看到。只有符合上述两个条件的才会被查询出来
##### INSERT
将当前系统版本号作为数据行快照的创建版本号。
##### DELETE
将当前系统版本号作为数据行快照的删除版本号。
##### UPDATE
将当前系统版本号作为更新前的数据行快照的删除版本号，并将当前系统版本号作为更新后的数据行快照的创建版本号。可以理解为先执行`DELETE`后执行`INSERT`。保存这两个版本号，使大多数操作都不用加锁。使数据操作简单，性能很好，并且能保证只会读取到复合要求的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作和一些额外的维护工作。

### 一致性非锁定读
一致性的非锁定读（`consistent nonlocking read`）是指InnoDB存储引擎通过行多版本控制（`multi versioning`）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB存储引擎会去读取行的一个快照数据。之所以称其为非锁定读，因为不需要等待访问的行上X锁的释放。快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。而undo用来在事务中回滚数据，因此快照数据本身是没有额外的开销。此外，读取快照数据是不需要上锁的，因为没有事务需要对历史的数据进行修改操作。
#### READ COMMITTED隔离级别的实现
对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据，违反了事务的隔离性
#### REPEATABLE READ隔离级别的实现
对于快照数据，非一致性读总是读取事务开始时的行数据版本。

### 一致性锁定读
在某些情况下，用户需要显式地对数据库读取操作进行加锁以保证数据逻辑的一致性。而这要求数据库支持加锁语句，即使是对于SELECT的只读操作。InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读（locking read）操作： 
* SELECT…FOR UPDATE
* SELECT…LOCK IN SHARE MODE

## 事务
### 什么是事务
事务（Transaction）是并发控制的基本单位。所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。

### 事务的四大特性
ACID
### MySQL事务实现机制
* 事务隔离性由锁来实现。
* 原子性、 一致性、 持久性通过数据库的`redo log`和`undo log`来完成。
#### redo log
重做日志用来实现事务的持久性，即D特性。它由两部分组成：
* 内存中的重做日志缓冲
* 重做日志文件

在事务提交时，必须先将该事务的所有日志写入到redo日志文件中，待事务的commit操作完成才算整个事务操作完成。在每次将`redo log buffer`写入`redo log file`后，都需要调用一次`fsync`操作，因为重做日志缓冲只是把内容先写入操作系统的缓冲系统中，并没有确保直接写入到磁盘上，所以必须进行一次fsync操作。因此，磁盘的性能在一定程度上也决定了事务提交的性能。

关于fsync这个操作用户是可以干预的，因为每次提交事务都执行一次fsync，确实影响数据库性能。通过`innodb_flush_log_at_trx_commit`来控制`redo log`刷新到磁盘的策略。该参数的默认值为1，表示每次提交事务时都执行一次fsync操作。0则表示事务提交时不进行写入重做日志文件，这个写入操作由master thread进程来完成，master thread每一秒会进行一次重做日志文件的fsync操作。2则表示事务提交时将重做日志写入重做日志文件，但仅写入文件系统的缓存中，并不进行fsync操作。用户可以通过设置0或者2啦提高事务提交的性能，也可以设置1来要求确保redo log是写入文件中的，总之三种方法各有利弊。

重做日志都是以512字节为单位进行存储的，这意味着重做日志缓存、重做日志文件块都是以块（`block`）的方式进行保存的，称为重做日志块(`redo log block`)。每块的大小512字节。由于重做日志块的大小和磁盘扇区大小一样，都是512字节，因此重做日志的写入可以保证原子性，不需要`double write`技术。

每个重做日志块的内容快除了日志记录本身之外，还由日志块头(`log block header`)及日志块尾(`log block tailer`)两部分组成。重做日志头一共占用12字节，重做日志尾占用8字节。这两部分是固定的。故每个重做日志块实际可以存储的大小为492字节(512-12-8)。

在应用重做日志进行恢复时，会根据日志中的日志序列号（`LSN-Log Sequence Number`）判断该重做日志页需不需要应用。LSN是`Log Sequence Number`的缩写，其代表的是日志序列号，在InnoDB存储引擎中，LSN占用8个字节，并且单调递增，表示事务写入重做日志字节的总量。例如当前重做日志的LSN为1000，有一个事务T1写入了100字节的重做日志，那么LSN就变成1100，若又有事务T2写入200字节的重做日志，那么LSN就变为1300。LSN不仅记录在重做日志中，还存在每个页中，在每个页的头部，有一个值FIL_PAGE_LSN，记录了该页的LSN，在页中，LSN表示该页最后刷新时LSN的大小。因为重做日志记录的是每个页的日志，因此页中的LSN可以判断页是否需要进行恢复操作。例如，页P1的LSN为10000，而数据库启动时，InnoDB检测到写入重做日志中的LSN为13000，并且事务已经提交，那么数据库需要进行恢复操作。将重做日志应用到P1页中，同样的，对于重做日志中LSN小于P1页的LSN，不需要进行重做，因为P1页中的LSN表示已经被刷新到该位置，在此位置之前的内容已经被成功的处理了。

InnoDB存储引擎在启动时不管上次数据运行是否正常关闭，都会尝试进行恢复操作，因为重做日志记录的是物理日志，因此恢复的速度比逻辑日志，如二进制日志要快的多，于此同时，InnoDB存储引擎自身也对恢复进行了一定程度的优化，如顺序读取及并行应用重做日志，这样可以进一步提高数据库恢复的速度

#### undo log
* 回滚日志实现事务的一致性。此外，事务回滚、MVCC也由`undo log`实现。
* `undo log`和`redo log`记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，`undo log`中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。
* 当执行回滚时，就可以从`undo log`中的逻辑记录读取到相应的内容并进行回滚。有时候应用到行版本控制的时候，也是通过`undo log`来实现的：当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，帮助用户实现一致性非锁定读取。

## SQL优化
### 如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？
对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。

### 大表数据查询，怎么优化
* 优化shema、sql语句+索引；
* 第二加缓存，memcached, redis；
* 主从复制，读写分离；
* 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；
* 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；

### 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？
* 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。
* 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。
* 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。

### 超大分页怎么处理？
超大的分页一般从两个方向上来解决。
* 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于`select * from table where age > 20 limit 1000000,10`这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为`select * from table where id in (select id from table where age > 20 limit 1000000,10)`。这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以`select * from table where id > 1000000 limit 10`，效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据.
* 从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页。只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击。解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可.

### 主键使用自增ID还是UUID？
* 推荐使用自增ID，不要使用UUID。因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。所以，在数据量大一些的情况下，用自增主键性能会好一些。
* 在InnoDB存储引擎表中，每张表都有个主键（Primary Key），如果在创建表时没有显式地定义主键，则InnoDB存储引擎会按如下方式选择或创建主键：首先判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，则该列即为主键；如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。

### 字段为什么要求定义为not null？
null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。

### 如果要存储用户的密码散列，应该使用什么字段进行存储？
密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。

## 数据库优化
### 数据库结构优化的方法
一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果，需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。常用方法如下：
#### 将字段很多的表分解成多个表（垂直分表）
对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。
#### 增加中间表
对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。
#### 增加冗余字段
设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。一般来说表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。但是需要注意，冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。

### MySQL数据库cpu飙升到500%的话该怎么处理？
当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。
* 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。
* 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等

## 分库分表
### 什么是分片
分片(Sharding)是一种与水平切分相关的数据库架构模式，将一个表里面的行，分成多个不同的表的做法（称为分区）。每个区都有相同的模式和列，但每个表有完全不同的行。同样，每个分区中保存的数据都是唯一的，并且与其他分区中保存的数据无关。
### 分片的优点 VS 缺点
优点：
* 可以添加更多的机器，允许更多的流量，实现水平扩展（向外扩展）。
* 加快查询响应时间，通过将一个表拆分成多个，查询过程中便利更少的行，并且返回结果集速度加快。
* 减少宕机的影响，对于分片数据库，宕机可能只会影响单个分片，使应用程序更加稳定可靠。

缺点（风险）：
* 复杂度提高：正确实现分片数据库架构，是十分复杂的。如果操作不当，分片过程可能会造成数据丢失或表损坏，风险大。同时，即使正确的进行了分片，也可能对团队工作流程产生重大影响。与从单个入口点入口点访问和管理数据不同，用户必须跨多个分片管理数据。
* 分片不平衡：不恰当的分片方式，会造成分片不平衡，得不到分片的好处。
* 架构难以回滚： 一档是数据库进行了分片，就很难在将其恢复到未分片架构


### 分库分表后面临的问题
#### 事务支持
分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。

#### 跨节点的join，count,order by,group by
这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。

#### 数据迁移，容量规划，扩容等问题 
设计数据库分片，必须要考虑到以后数据迁移，容量规划，扩容等问题。一般来说，要做好数据库预分片（见Redis篇）。

#### ID问题
一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由

### 分片算法
一个好的分片算法，应该具备以下特点
* 分布均匀，即每个分片的数据量要尽可能相近。
* 负载均衡，及每台设备上的请求量要尽可能相近。
* 扩缩容时产生的数据迁移尽可能少。
#### 范围分片
一般适用于key为整形的情况，每台设备上存放相同大小的号段区间。

优点：
* 实现简单

缺点：
* 数据可能分布不均匀，如小号段数据量可能比大号段数据量要大
* 各个号段数据热度可能不一致，导致各个设备负载不均衡
#### 索引分片（这是路由的方法吧？？）
在检索表中存储key值和分片的映射关系，通过查找检索表来查找数据所在的分片，确定数据分布。

优点：
* 比较灵活：可以对每个key都存储映射关系，也可以结合划分号段的方式来减小检索表的容量。

缺点：
* 每次查询或写入都要连接到检索表，可能会对应用程序性能产生不利影响。同时，检索表如果出现单点故障，也会影响数据库写入新数据和访问现有数据的能力。

#### 哈希分片
计算key的哈希值，然后对节点数量取模。

优点：
* 实现简单
* 数据分布均匀，热点数据均匀

缺点：
* 扩缩容会产生大量的数据迁移
#### 一致性哈希分片
1. 一致性哈希算法将整个哈希值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织
2. 将各个服务器进行hash，一个服务器节点可以计算成多个哈希值，每个哈希值都可以在圆环相应的位置防止一个虚拟节点
3. 计算key的哈希值，沿环顺时针方向行走，遇到的第一个节点就是其该定位到的服务器。

优点：
* 实现简单
* 数据分布均匀
* 扩缩容数据迁移较少

#### 如何判断是否应该分片
究竟该不该采用分片的数据库架构一直是备受争议的问题。一些人认为当数据库达到一定量级的时候，采用分片架构是必然的；另一些人则认为由于操作复杂，除非是万不得已，否则不应该使用分片架构。

由于其复杂性，数据库分片架构多用于处理非常大量的数据。以下这些场景中，对数据库分片可能会非常有用：
* 应用数据不断增长，超出了单点数据库的存储能力。
* 数据库的读写超出了单点数据库或只读从库（读写分离架构下）的处理能力，从而导致了响应慢或超时。
* 应用所需的网络带宽超出了单点数据库或只读从库的可用带宽，从而导致了响应慢或超时。

对数据库分片之前，你最好先尝试其他的方式来优化你的数据库。比如：
* 使用远程数据库。如果你有一个庞大的应用，其所有的组件都依赖于同一个数据库服务器，你可以考虑将数据库迁移到一台单独的机器上来提高其性能。这不会像数据库分片那样复杂，因为所有的数据库表都还是完整的。并且，这种方式还允许你能够抛开其他基础设施，单独地对数据库做纵向扩展。
* 使用缓存。如果应用受制于读数据的性能，使用缓存是改进性能的一种方式。缓存通过将请求过的数据暂时地保存在内存中，加速后续的数据访问。
* 创建若干个只读从库。另一种能够改进读取性能的方法是将数据从主库拷贝到若干个从库中。这样一来，新的写操作将使用主库，之后再拷贝到从库中，而读操作将使用从库。这种读写分离的模式使得每一台机器都不会承受过大的负载，有助于防止机器变慢甚至崩溃。值得注意的是，创建只读从库需要更多的计算资源、花更多的钱，某些情况下，这些将会使你步履维艰。
* 升级服务器。一些情况下，升级数据库服务器的配置比数据库分片简单多了。对比创建只读从库，升级服务器配置可能会花更多的钱。因此，除非这真的是你最好的选择，否则不应该对机器扩容。

谨记只有当你的应用或者网站增长超过了一定量级的时候，以上这些方法都不足以有效地改进其性能的时候，数据库分片才真的是最佳选择。

## 复制
### 复制原理
1. 主服务器（master）把数据更改记录到二进制日志（binlog）中。
2. 从服务器（slave）把主服务器的二进制日志复制到自己的中继日志（relay log）中。
3. 从服务器重做中继日志中的日志，把更改应用到自己的数据库上，以达到数据的最终一致性。

### MySQL主从复制解决的问题
* 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份
* 负载均衡：降低单个服务器的压力
* 高可用和故障切换：帮助应用程序避免单点失败
* 升级测试：可以用更高版本的MySQL作为从库

## 备份
### mysqldump&xtrabackup

## 参考链接
* <https://juejin.im/post/5cb6c4ef51882532b70e6ff0>
* <https://blog.csdn.net/ThinkWon/article/details/104778621>
* <https://www.cnblogs.com/linjiqin/archive/2012/04/01/2428695.html>
* <https://www.zhihu.com/question/20596402>
* <https://blog.csdn.net/u010870518/article/details/79450295>
* <https://zhuanlan.zhihu.com/p/48327345>
* <https://juejin.im/post/5cd8283ae51d453a907b4b29#heading-1>
* <https://blog.csdn.net/abigale1011/article/details/6450845>
* <https://zhuanlan.zhihu.com/p/57185574>
* <https://blog.csdn.net/shmnh/article/details/73522409>