# 操作系统-IO篇

## 概念说明
* 用户空间与内核空间
* 进程切换
* 进程阻塞
* 文件描述符
* 缓存IO
* 自缓存应用程序
* 直接IO
* IO模型和IO多路复用

### 用户空间与内核空间
现代操作系统都采用虚拟存储器，对于32位操作系统而言，寻址空间（虚拟存储空间）为4GB。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证内核安全，操作系统将虚拟存储空间划分为两部分，一部分为内核空间，一部分为用户空间。针对Linux操作系统而言，地址最高的1GB字节（从虚拟地址0xC0000000到0xFFFFFFFF)供内核使用，成为内核空间，较低的3GB字节（从虚拟地址0x00000000到0xBFFFFFFF)供各个进程使用，成为用户空间。

### 进程切换
为了控制进程的执行，内核必须要有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下执行的，是与内核紧密相关的。从一个进程的运行切换到另一个进程，需要
1. 保存处理机上下文，包括程序计数器和寄存器
2. 更新PCB信息
3. 把进程PCB移入相应的队列，如就绪、阻塞队列
4. 选择另一个进程的执行，并更新PCB
5. 更新内存管理的数据结构
6. 恢复处理机上下文

总而言之就是很耗资源

### 进程的阻塞
正在执行的进程，由于期待某些事件的发生，如请求系统资源失败、等待某种操作的完成，新数据尚未到达等，则由系统执行阻塞原语，从运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程，才能将其转为阻塞态。当进程进入阻塞态时，是不占用CPU资源的。

### 文件描述符
在Linux系统中，一切都是文件。文件描述符就是当程序打开一个现有文件或者创建一个新文件时，内核向进程返回值。

### 缓存IO
缓存I/O又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。
* 读操作：操作系统检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。
* 写操作：将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync同步命令

#### 优点
* 在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全；
* 可以减少读写盘的次数，从而提高性能。\
当应用程序尝试读取某块数据的时候，如果这块数据已经存放在了页缓存中，那么这块数据就可以立即返回给应用程序，而不需要经过实际的物理读盘操作。当然，如果数据在应用程序读取之前并未被放到页缓存中，那么就需要先将数据从磁盘读到页缓存中。对于写操作来说，应用程序也会将数据写到页缓存中去，数据是否立即写到磁盘上去取决于应用程序所采用的写机制：如果用户采用的是同步写机制，那么数据会立即写到磁盘上去，应用程序一直等待到数据写完为止；如果用户才用的是延迟写机制，那么应用程序不需要等待数据全部被写回到磁盘，数据只要写到页缓存中去就可以了。在延迟写机制的情况下，操作系统会定期将放在页缓存中的数据刷到磁盘上去。与异步写机制不同的是，延迟写机制在数据完全写到磁盘上的时候不会通知应用程序，而异步写机制在数据完全写到磁盘上的时候会返回给应用程序的。所以延迟写机制本身存在丢数据的风险，而异步写机制则不会。

#### 缺点
* 在缓存 I/O 机制中，DMA 方式可以将数据直接从磁盘读到页缓存中，或者将数据从页缓存直接写回到磁盘上，而不能直接在应用程序地址空间和磁盘之间进行数据传输，这样，数据在传输过程中需要在应用程序地址空间（用户空间）和缓存（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。

### 自缓存应用程序
对于某些应用程序来说，它会有自己的数据缓存机制，比如，它会将数据缓存在应用程序的地址空间，这类应用程序完全不需要使用操作系统内核中的告诉缓冲存储器，这类应用程序就被称作自缓存应用程序，比如数据库管理系统。自缓存应用需要对操作的数据语义了如指掌，所以它可以采用更高效的缓存替换算法。

### 直接IO
通常来说，直接IO需要跟异步IO结合起来使用

#### 优点
* 减少操作系统内核缓冲区和应用程序地址空间的数据拷贝次数，降低了对文件读写所带来的CPU使用和内存带宽的占用。

#### 缺点
* 设置直接IO的开销非常大
* 读操作: 直接IO的读操作会造成磁盘同步读。这可能会导致进程需要较长时间才能执行完。
* 写操作：直接IO需要write()系统调用同步执行，也可能会导致应用程序关闭缓慢。

## IO模式
当一个read操作发生时，需要经历两个阶段
1. 等待数据准备
2. 将数据从内核拷贝到进程中

因此,Linux系统产生了以下五种IO模式
1. 阻塞IO
2. 非阻塞IO
3. IO多路复用
4. 信号驱动IO
5. 异步IO

### 阻塞IO
当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。\
**所以，blocking IO的特点就是在IO执行的两个阶段都被block了。**

### 非阻塞IO
当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。\
**所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。**

### 异步IO
用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。

### 总结

#### 同步&异步
同步和异步关注的是**消息通信机制**，所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。而异步则是在调用发出后，调用直接就返回了。

#### 阻塞&非阻塞
阻塞和非阻塞关注的是程序在**等待调用结果时的状态**。阻塞指在调用结果返回前，当前线程会被挂起，调用线程只有得到结果之后才会返回。非阻塞指在不能立刻得到结果之前，该调用不会阻塞当前线程。\
以小明下载文件为例：
* 同步阻塞：小明一直盯着下载进度条，知道100%下载完成
* 同步非阻塞：小明点击下载后就玩手机去了，不过每隔一段时间会过来check下载是否完成
* 异步阻塞：小明有个下载完成通知的软件，在下载完成后会“叮”的一声通知小明下载完成，不过小明一直傻傻的等待“叮”的声音
* 异步非阻塞：仍然是那个下载完成会“叮”的一声通知的软件，不过小明提交下载任务后就去做别的事情去了，当听到“叮”的声音就知道下载完成了。

### IO多路复用（事件驱动）（同步IO）
多路复用在通信上指的是在一个信道上传输多路信号或数据流的技术。\
IO多路复用指的是通过某种机制，监听多个文件描述符（socket)，当其中任意一个文件描述符处于就绪状态时，能够通知程序进行相应的读写操作。

#### Linux（2.6+） socket的wakeup callback机制（IO多路复用机制的本质）
Linux通过socket睡眠队列来管理所有等待socket的某个事件的进程（Process），同时通过**wakeup**机制来异步唤醒整个睡眠队列上等待时间的进程，通知进程相关事件的发生。\
我们可以理解为，每隔socket维护了一个队列，比如socket可读的时候，内核就会唤醒队列中的各个进行，并且执行每个进程的**callback**函数。 总体上涉及三个逻辑
* 睡眠逻辑：select、poll、epoll_wait陷入内核，判断监控的socket是否有关心的事件发生，如果没有，则为当前进程创建一个wait_entry节点，然后插入到监控socket的sleep_list里去。
* schedule函数：shcedule函数是Linux的调度Process的函数，Linux调用schedule函数，管理进程的状态转换，进程处于sleep状态直到超时或者关心的事件发生。
* 唤醒逻辑：socket事件发生后，socket将按顺序遍历其睡眠队列sleep_list，依次调用每个wait_entry节点的callback函数，直到完成队列遍历或遇到某个排他类型的wait_entry节点为止。

#### 为啥需要IO多路复用
对于阻塞的IO来说，当调用read、write执行操作时，如果数据还没准备好，那么该线程就会被挂起，直到数据准备好。\
对于服务器来说，假如它需要处理1000个连接，但这1000个连接又只有很少连接是忙碌的，则这1000个线程大部分时间是处于阻塞状态的。由于CPU的核数或超线程数一般较小，那么每个线程分配得到的时间片也就比较少，线程切换频繁。这样子是有问题的
* 线程是有内存开销的，1个线程可能需要512K存放栈，那么1000个线程就需要512M内存。
* 线程切换是需要消耗CPU时间的。当大量时间花在上下文切换的时候，分配给真正的操作的CPU时间就要少很多。
  
#### select
在一个高性能的网络服务上，大多情况下一个服务进程需要同时处理多个socket，我们需要公平地对待所有的socket。对于read而言，哪个socket刻度，进程就应该去读取该socket的数据来处理。于是对于read操作，一个朴素的需求就是关心N个socket是否有数据可读，也就是我们期待可读事件的通知，而不是盲目的对每个socket调用recv/recvfrom来尝试接收数据。我们应该block在等待事件的发生上，这个事件简单点就是关心N个socket中一个或多个socket有数据可读，当block解除时，就意味着，我们一定可以找到一个或多个socket上有可读的数据。另一方面，根据上面的socket wakeup callback机制，我们不知道什么时候，哪个socket会有读事件发生，于是，进程需要同时插入这N个socket的sleep_list上等待任意一个socket可读事件发生而唤醒。当进程被唤醒时，callback里面应该有个逻辑去检查哪些socket可读了。\
于是，select的多路复用逻辑就清晰了，select为每一个socket引入一个poll逻辑，该poll逻辑用于收集socket发生的事件，对于可读事件，简单伪代码如下
```c
poll() {
    // 其他逻辑
    if (receive queue is not empty) {
        sk_event |= POLL_IN;
    }
    // 其他逻辑
}
```
当用户进程调用select的时候，select会讲需要监控的socket集合拷贝到内核空间，然后遍历自己监控的socket集合，挨个调用poll逻辑检查该socket是否有可读事件，遍历完所有的socket后，如果没有任何一个socket刻度，那么select将调用schedule_timeout进入schedule循环，是的进程进入睡眠。如果在timeout事件内某个socket可读了，或者等待timeout了，则调用socket的进程会被唤醒，接下来select就会遍历监控的socket集合，挨个手机可读事件并返回给用户。相应伪代码如下
```c
for (sk in readfds) {
    sk_event.evt = sk.poll()
    sk_event.sk = sk;
    ret_event_for_process;
}
```
通过上面的select逻辑过程分析，可知select存在两个问题+一个限制问题
1. 为了减少数据拷贝带来的性能损坏，内核对监控空间的fds集合大小做了限制，并且这个是通过宏控制的，大小不可改变（限制为1024）。我们希望有个更大的可监控集合。
2. 被监控的fds(socket集合)需要从用户空间拷贝到内核空间，我们不希望拷贝
3. 被监控的fds集合中，只要有一个有数据可读，整个socket集合就会被遍历一次调用对应的poll函数收集可读事件。我们希望能直接从通知中得到有可读事件的socket列表，而不是需要遍历一次。


#### poll（鸡肋）
select遗留的三个问题中，问题1是用法限制问题，问题2和3则是性能问题。poll和select非常相似，poll并没有着手解决性能问题，poll只解决了select的问题1中fds集合大小1024的限制问题。\
poll改进： 修改了fds集合的描述方式，是的支持集合大小远大于select的1024.

#### epoll
对于IO多路复用，用两件事情必须要做（对于监控可读事件而言）：
1. 准备好监控的fds集合
2. 探测并返回fds集合中那些fd可读了

细看select或poll，都在重复地准备整个需要监控的fds集合。然而对于频繁调用select或poll而言，fds集合的变化频率要低很多，我们没必要每次都重新准备整个fds集合。\
于是，epoll引入了epoll_ctl系统调用，将高频调用的epoll_wait和低频的epoll_ctl隔离开。同时，epoll_ctrl通过(EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL)三个操作来分散对需要监控的fds集合的修改，做到了有变化才变更，将select或poll高频、大块内存拷贝变为epoll_ctl的低频、小内存拷贝。同时，对于高频的epoll_wait的可读就绪的fd集合返回的拷贝问题，epoll通过内核与用户控件的mmap（内存映射）同一块内存来解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到同一块物理内存地址，使得这块物理内存对内核和用户均可见，减少用户态和内核态的数据交换。\
另外，epoll通过高效的数据结构使epoll_ctl对fds集合的增删改查操作变得快速。Linux 2.6.8之前的内核，epoll使用hash来组织fds集合，2.6.8之后使用红黑树来组织fds集合。

##### 按需遍历就绪的fds集合
通过上面的socket的睡眠队列唤醒逻辑可知，socket唤醒睡眠在其睡眠队列的wait_entry(process)的时候会调用wait_entry的回调函数callback，并且，我们可以在callback中做任何事情。为了做到只遍历就绪的fd，我们需要有个地方来组织那些已就绪的fd。为此，epoll引入了一个中间层，一个双向链表(ready_list)，一个单独的睡眠队列(single_epoll_wait_list)。并且，与select或poll不同的是，epoll的process不需要同时插入到多路复用的socket集合的所有睡眠队列上；相反，process值需要插入到中间层epoll的单独睡眠队列中，process睡眠在epoll的单独队列上，等待事件的发生。同时，引入一个中间的wait_entry_sk，他与某个socket密切相关，wait_entry_sk睡眠在socket的睡眠队列上，其callback函数逻辑是将当前socket排入到epoll的ready_list中，并唤醒epoll的single_epoll_wait_list。而single_epoll_wait_list上睡眠的process的回调函数就明朗了，遍历ready_list上的所有sccket，挨个调用poll函数收集事件，然后唤醒process从epoll_wait返回。\
epoll巧妙之处在于引入了一个中间层解决了大量监控socket无效遍历问题，在中间层上为每个监控的socket准备一个单独的回调函数epoll_callback_sk，而对于select/poll，所有的socket都共用一个相同的回调函数。正是这个单独的回调epoll_callback_sk使得每个socket都能单独处理自身，当自己就绪时将自身socket挂入到epoll的ready_list。同时，epoll引入了一个睡眠队列single_epoll_wait_list，分割了两类等待。process不在睡眠在所有的socket的睡眠队列上，而是睡眠在epoll的睡眠队列上，在等待“任意一个socket可读就绪”事件。而中间wait_entry_sk则代替process睡眠在具体的socket上，当socket就绪时，它就可以处理自身了。

##### ET（Edge Triggered 边沿触发） vs LT（level Triggered 水平触发）
ET 边沿触发：
1. 接收缓冲buffer内待读数据增加的时候，触发度事件（由空变为不空，或者有新数据进入缓冲区buffer）
2. 调用epoll_ctl(EPOLL_CTL_MOD)来改变socket_fd的监控事件，也就是重新mod socket_fd的EPOLLIN事件，并且接收缓冲buffer内还有数据没读取。（这里不能是EPOLL_CTL_ADD的原因是epoll不允许重复ADD，除非先DEL，再ADD）
3. 发送buffer内待发送的数据减少的时候（由满状态变为不满状态，或者有部分数据发送出去）
4. 调用epoll_ctl(EPOLL_CTL_MOD)来改变socket_fd的监控事件，也就是重新mod socket_fd的EPOLLOUT事件，并且发送缓冲buffer内还有数据还没满。（这里不能是EPOLL_CTL_ADD的原因是epoll不允许重复ADD，除非先DEL，再ADD）

LT 水平触发：
1. 接收缓冲buffer内有数据可读，则读事件一直触发
2. 发送缓冲区不满可以继续写入数据，则写事件一直触发

ready_list移除：
* ET 边沿触发： 遍历epoll的ready_list，将sk从ready_list中移除，然后调用该sk的poll逻辑收集发生的事件
* LT 水平触发： 遍历epoll的ready_list，将sk从ready_list中移除，然后调用该sk的poll逻辑收集发生的事件，如果该sk的poll函数返回了关心的事件，那么该sk将被重新加入到epoll的ready_list中。

###### ET VS LT - 性能比较
通过上面的概念介绍，我们知道对于可读事件而言，LT比ET多了两个操作：
1. 对ready_list的遍历的时候，对于收集到可读事件的sk会重新放入ready_list
2. 下次epoll_wait的时候会再次遍历上次重新放入的sk，如果sk本身没有数据可读了，那么这次遍历就变得多余了

在服务端有海量活跃socket的时候，LT模式下，epoll_wait返回的时候，会有海量的socket sk重新放入ready_list。如果，用户在第一次epoll_wait返回的时候，将有数据的socket都处理掉了，那么下次epoll_wait的时候，上次epoll_wait重新入ready_list的sk被再次遍历就有点多余，这个时候LT确实会带来一些性能损失。然而，实际上会存在很多多余的遍历么？\
先不说第一次epoll_wait返回的时候，用户进程能否都将有数据返回的socket处理掉。在用户处理的过程中，如果该socket有新的数据上来，那么协议栈发现sk已经在ready_list中了，那么就不需要再次放入ready_list，也就是在LT模式下，对该sk的再次遍历不是多余的，是有效的。同时，我们回归epoll高效的场景在于，服务器有海量socket，但是活跃socket较少的情况下才会体现出epoll的高效、高性能。因此，在实际的应用场合，绝大多数情况下，ET模式在性能上并不会比LT模式具有压倒性的优势，至少，目前还没有实际应用场合的测试表面ET比LT性能更好。

###### ET vs LT - 复杂度比较
我们知道，对于可读事件而言，在阻塞模式下，是无法识别队列空的事件的，并且，事件通知机制，仅仅是通知有数据，并不会通知有多少数据。于是，在阻塞模式下，在epoll_wait返回的时候，我们对某个socket_fd调用recv或read读取并返回了一些数据的时候，我们不能再次直接调用recv或read，因为，如果socket_fd已经无数据可读的时候，进程就会阻塞在该socket_fd的recv或read调用上，这样就影响了IO多路复用的逻辑(我们希望是阻塞在所有被监控socket的epoll_wait调用上，而不是单独某个socket_fd上)，造成其他socket饿死，即使有数据来了，也无法处理。\
接下来，我们只能再次调用epoll_wait来探测一些socket_fd，看是否还有数据可读。在LT模式下，如果socket_fd还有数据可读，那么epoll_wait就一定能够返回，接着，我们就可以对该socket_fd调用recv或read读取数据。然而，在ET模式下，尽管socket_fd还是数据可读，但是如果没有新的数据上来，那么epoll_wait是不会通知可读事件的。这个时候，epoll_wait阻塞住了，这下子坑爹了，明明有数据你不处理，非要等新的数据来了在处理，那么我们就死扛咯，看谁先忍不住。\
等等，在阻塞模式下，不是不能用ET的么？是的，正是因为有这样的缺点，ET强制需要在非阻塞模式下使用。在ET模式下，epoll_wait返回socket_fd有数据可读，我们必须要读完所有数据才能离开。因为，如果不读完，epoll不会在通知你了，虽然有新的数据到来的时候，会再次通知，但是我们并不知道新数据会不会来，以及什么时候会来。由于在阻塞模式下，我们是无法通过recv/read来探测空数据事件，于是，我们必须采用非阻塞模式，一直read直到EAGAIN。因此，ET要求socket_fd非阻塞也就不难理解了。\
另外，epoll_wait原本的语意是：监控并探测socket是否有数据可读(对于读事件而言)。LT模式保留了其原本的语意，只要socket还有数据可读，它就能不断反馈，于是，我们想什么时候读取处理都可以，我们永远有再次poll的机会去探测是否有数据可以处理，这样带来了编程上的很大方便，不容易死锁造成某些socket饿死。相反，ET模式修改了epoll_wait原本的语意，变成了：监控并探测socket是否有新的数据可读。\
于是，在epoll_wait返回socket_fd可读的时候，我们需要小心处理，要不然会造成死锁和socket饿死现象。典型如listen_fd返回可读的时候，我们需要不断的accept直到EAGAIN。假设同时有三个请求到达，epoll_wait返回listen_fd可读，这个时候，如果仅仅accept一次拿走一个请求去处理，那么就会留下两个请求，如果这个时候一直没有新的请求到达，那么再次调用epoll_wait是不会通知listen_fd可读的，于是epoll_wait只能睡眠到超时才返回，遗留下来的两个请求一直得不到处理，处于饿死状态。

## 参考链接
* <https://cloud.tencent.com/developer/article/1005481>
* <https://www.ibm.com/developerworks/cn/linux/l-cn-directio>
* <https://segmentfault.com/a/1190000003063859>
